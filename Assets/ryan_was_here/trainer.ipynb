{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from Discrete_SAC_Agent import SACAgent\n",
    "\n",
    "TRAINING_EVALUATION_RATIO = 4\n",
    "RUNS = 5\n",
    "EPISODES_PER_RUN = 400\n",
    "STEPS_PER_EPISODE = 200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    agent_results = []\n",
    "    for run in range(RUNS):\n",
    "        agent = SACAgent(env)\n",
    "        run_results = []\n",
    "        for episode_number in range(EPISODES_PER_RUN):\n",
    "            print('\\r', f'Run: {run + 1}/{RUNS} | Episode: {episode_number + 1}/{EPISODES_PER_RUN}', end=' ')\n",
    "            evaluation_episode = episode_number % TRAINING_EVALUATION_RATIO == 0\n",
    "            episode_reward = 0\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done and i < STEPS_PER_EPISODE:\n",
    "                i += 1\n",
    "                action = agent.get_next_action(state, evaluation_episode=evaluation_episode)\n",
    "                next_state, reward, done, _, info = env.step(action)\n",
    "                if not evaluation_episode:\n",
    "                    agent.train_on_transition(state, action, next_state, reward, done)\n",
    "                else:\n",
    "                    episode_reward += reward\n",
    "                state = next_state\n",
    "            if evaluation_episode:\n",
    "                run_results.append(episode_reward)\n",
    "        agent_results.append(run_results)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    n_results = EPISODES_PER_RUN // TRAINING_EVALUATION_RATIO\n",
    "    results_mean = [np.mean([agent_result[n] for agent_result in agent_results]) for n in range(n_results)]\n",
    "    results_std = [np.std([agent_result[n] for agent_result in agent_results]) for n in range(n_results)]\n",
    "    mean_plus_std = [m + s for m, s in zip(results_mean, results_std)]\n",
    "    mean_minus_std = [m - s for m, s in zip(results_mean, results_std)]\n",
    "\n",
    "    x_vals = list(range(len(results_mean)))\n",
    "    x_vals = [x_val * (TRAINING_EVALUATION_RATIO - 1) for x_val in x_vals]\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim([0, 200])\n",
    "    ax.set_ylabel('Episode Score')\n",
    "    ax.set_xlabel('Training Episode')\n",
    "    ax.plot(x_vals, results_mean, label='Average Result', color='blue')\n",
    "    ax.plot(x_vals, mean_plus_std, color='blue', alpha=0.1)\n",
    "    ax.plot(x_vals, mean_minus_std, color='blue', alpha=0.1)\n",
    "    ax.fill_between(x_vals, y1=mean_minus_std, y2=mean_plus_std, alpha=0.1, color='blue')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
