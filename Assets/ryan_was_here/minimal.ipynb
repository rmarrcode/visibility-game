{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class GridEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(GridEnv, self).__init__()\n",
    "        self.grid_size = 10\n",
    "        self.action_space = spaces.Discrete(4)  # Actions: 0=Up, 1=Right, 2=Down, 3=Left\n",
    "        self.observation_space = spaces.Box(low=0, high=self.grid_size - 1, shape=(2,), dtype=np.int32)\n",
    "        self.state = np.zeros(2, dtype=np.int32)\n",
    "        self.goal = np.array([self.grid_size - 1, self.grid_size - 1], dtype=np.int32)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.array([0, 0], dtype=np.int32)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0 and self.state[0] > 0:  # Up\n",
    "            self.state[0] -= 1\n",
    "        elif action == 1 and self.state[1] < self.grid_size - 1:  # Right\n",
    "            self.state[1] += 1\n",
    "        elif action == 2 and self.state[0] < self.grid_size - 1:  # Down\n",
    "            self.state[0] += 1\n",
    "        elif action == 3 and self.state[1] > 0:  # Left\n",
    "            self.state[1] -= 1\n",
    "        \n",
    "        done = np.array_equal(self.state, self.goal)\n",
    "        reward = 1.0 if done else -0.1\n",
    "        \n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rmarr\\Documents\\python-envs\\3.7.0\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mean = nn.Linear(256, action_dim)\n",
    "        self.fc_log_std = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_log_std(x)\n",
    "        return mean, log_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_q = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        print(f'state {state}')\n",
    "        print(f'action {action}')\n",
    "        x = torch.cat([state, action.unsqueeze(0)], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.fc_q(x)\n",
    "        return q_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_value = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.fc_value(x)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, tau=0.005, alpha=0.2):\n",
    "        self.actor = PolicyNetwork(state_dim, action_dim)\n",
    "        self.actor_target = PolicyNetwork(state_dim, action_dim)\n",
    "        self.q1 = QNetwork(state_dim, action_dim)\n",
    "        self.q2 = QNetwork(state_dim, action_dim)\n",
    "        self.q1_target = QNetwork(state_dim, action_dim)\n",
    "        self.q2_target = QNetwork(state_dim, action_dim)\n",
    "        self.value = ValueNetwork(state_dim)\n",
    "        self.value_target = ValueNetwork(state_dim)\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.q1_optimizer = torch.optim.Adam(self.q1.parameters(), lr=lr)\n",
    "        self.q2_optimizer = torch.optim.Adam(self.q2.parameters(), lr=lr)\n",
    "        self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self._update_target_networks()\n",
    "\n",
    "    def _update_target_networks(self):\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.q1_target.parameters(), self.q1.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.q2_target.parameters(), self.q2.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.value_target.parameters(), self.value.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        mean, log_std = self.actor(state)\n",
    "        std = torch.exp(log_std)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        action = torch.argmax(action)\n",
    "        return action.squeeze(0).detach().numpy()\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size=64):\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "        \n",
    "        # Update Q networks\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_std = self.actor_target(next_states)\n",
    "            next_std = torch.exp(next_log_std)\n",
    "            next_dist = torch.distributions.Normal(next_actions, next_std)\n",
    "            next_actions = next_dist.rsample()\n",
    "            target_q1 = self.q1_target(next_states, next_actions)\n",
    "            target_q2 = self.q2_target(next_states, next_actions)\n",
    "            target_value = torch.min(target_q1, target_q2) - self.alpha * next_log_std\n",
    "            target = rewards + (1 - dones) * self.gamma * target_value\n",
    "        \n",
    "        q1_value = self.q1(states, actions)\n",
    "        q2_value = self.q2(states, actions)\n",
    "        \n",
    "        q1_loss = F.mse_loss(q1_value, target)\n",
    "        q2_loss = F.mse_loss(q2_value, target)\n",
    "        \n",
    "        self.q1_optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.q1_optimizer.step()\n",
    "        \n",
    "        self.q2_optimizer.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.q2_optimizer.step()\n",
    "        \n",
    "        # Update Value Network\n",
    "        value = self.value(states)\n",
    "        value_loss = F.mse_loss(value, target_value.detach())\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        # Update Policy Network\n",
    "        actions, log_std = self.actor(states)\n",
    "        dist = torch.distributions.Normal(actions, torch.exp(log_std))\n",
    "        log_prob = dist.log_prob(actions).sum(dim=-1, keepdim=True)\n",
    "        q1_value = self.q1(states, actions)\n",
    "        q2_value = self.q2(states, actions)\n",
    "        q_value = torch.min(q1_value, q2_value)\n",
    "        policy_loss = (self.alpha * log_prob - q_value).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update Target Networks\n",
    "        self._update_target_networks()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Total Reward: -18.0\n",
      "Episode 2/1000, Total Reward: -3.4000000000000004\n",
      "Episode 3/1000, Total Reward: -25.600000000000108\n",
      "Episode 4/1000, Total Reward: -14.399999999999961\n",
      "Episode 5/1000, Total Reward: -4.4999999999999964\n",
      "Episode 6/1000, Total Reward: -11.199999999999973\n",
      "Episode 7/1000, Total Reward: -10.099999999999977\n",
      "tensor([[9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [8., 2.],\n",
      "        [8., 2.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [8., 2.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [8., 2.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [8., 2.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.]])\n",
      "tensor([[ -0.6831,   1.6161,  -1.4512,   2.4131],\n",
      "        [  0.2070,   2.5466,   0.8655,  -5.5019],\n",
      "        [  0.0140,   2.7420,  -1.0964,   5.1750],\n",
      "        [ -1.1305,   1.0967,   0.5375,  -2.2680],\n",
      "        [  0.6048,   1.5697,  -1.3048,   6.6291],\n",
      "        [ -1.3254,   1.2734,   3.6913,   6.4550],\n",
      "        [ -0.5265,  -1.3465,  -0.0597,   3.0982],\n",
      "        [  0.1364,   0.3088,   1.3260,   0.4588],\n",
      "        [ -1.2138,   2.2182,   1.1887,  -7.4326],\n",
      "        [ -0.4694,   1.3663,  -1.4939,  -6.6442],\n",
      "        [ -0.8115,   1.2796,  -1.0145,  -1.1242],\n",
      "        [ -1.0644,   1.8576,   0.8847,  -0.9838],\n",
      "        [ -1.1681,   3.6351,   2.5517,  -9.9371],\n",
      "        [ -0.5747,   2.1950,  -0.7413,   5.8082],\n",
      "        [ -1.5173,   3.7015,  -0.2329,   2.4801],\n",
      "        [  0.0149,   1.8640,  -0.6661,   2.0483],\n",
      "        [ -0.3234,   1.9528,   1.0984,  -2.7132],\n",
      "        [ -0.2276,   2.9995,   1.6487,   8.7644],\n",
      "        [ -0.9941,   3.0162,   1.0331,   1.6072],\n",
      "        [ -0.2315,   0.9521,  -0.2542,   3.7976],\n",
      "        [ -1.7600,  -0.7209,   0.9544,   0.7198],\n",
      "        [ -0.7777,   4.4262,   1.7277,  -1.8575],\n",
      "        [ -0.3815,   3.1127,  -0.4026,   4.7234],\n",
      "        [ -1.4068,   1.7496,  -0.1761,   5.9494],\n",
      "        [ -0.6072,   1.4619,   1.1835,  -4.0491],\n",
      "        [ -0.9485,  -0.6568,   0.1139,  -0.4580],\n",
      "        [ -0.2148,   2.6125,   1.5838,  -4.0227],\n",
      "        [  0.4475,   3.2074,   1.0985,  -3.6138],\n",
      "        [ -0.4815,   2.8973,   1.0824, -11.2952],\n",
      "        [  0.5571,   0.9513,   0.8421,  -3.0193],\n",
      "        [ -0.5033,   3.2166,  -0.6058,  -4.8095],\n",
      "        [ -0.9217,  -0.7387,   1.7524,  -2.4206],\n",
      "        [ -0.7194,   2.9384,   0.7601,   8.4003],\n",
      "        [ -1.3533,   3.3209,   0.9174,   8.9810],\n",
      "        [ -0.6086,  -3.4745,  -1.1780,   2.8173],\n",
      "        [  0.1049,   0.7786,   3.6920,  -5.0267],\n",
      "        [ -1.0117,   1.9546,  -1.8403,   3.6416],\n",
      "        [  0.6441,   4.6475,  -0.3219,   2.7792],\n",
      "        [ -1.2814,  -0.0452,   1.3392,   4.0244],\n",
      "        [ -0.5710,   0.8462,   0.2676,   2.9665],\n",
      "        [ -0.3575,   0.8002,   0.5219,  -0.0572],\n",
      "        [ -0.9981,   4.4347,   1.8256,  -6.5599],\n",
      "        [ -0.1277,   0.3297,   0.0987,  -3.1330],\n",
      "        [ -0.7945,   2.5868,  -0.1731,  -7.2968],\n",
      "        [ -1.0389,   3.7750,  -0.7730,  -3.4116],\n",
      "        [ -0.3775,   1.8967,   0.5613,  -3.8975],\n",
      "        [  0.2745,   0.0338,  -0.4437,  -8.3627],\n",
      "        [ -0.6390,   4.5584,  -0.8059,  -3.0687],\n",
      "        [ -1.3563,  -2.1902,   1.0950,  -2.9530],\n",
      "        [ -0.2439,  -1.1710,   0.7298,  -7.2243],\n",
      "        [  0.8482,   1.9445,   1.0523,  -8.0734],\n",
      "        [  0.1286,   1.6364,   0.7313,  -0.7272],\n",
      "        [ -0.1512,   1.7268,  -1.1847,  -2.4942],\n",
      "        [ -1.2759,   2.3352,   1.5472,   5.3895],\n",
      "        [  0.1650,   0.0834,   2.9445,  -1.9420],\n",
      "        [ -0.0233,   0.4722,   2.4161,   2.4121],\n",
      "        [  0.0455,   1.0940,   0.1269,   1.3251],\n",
      "        [ -0.5415,   2.5685,  -1.3263,   0.7908],\n",
      "        [ -0.7313,  -0.1797,  -0.6803,   3.3483],\n",
      "        [  1.0770,   3.3452,   0.2337,  -1.5340],\n",
      "        [ -0.6377,   1.6484,   0.9340,   1.0218],\n",
      "        [ -0.3922,   3.5201,   0.3311,  -2.6052],\n",
      "        [ -0.8038,   1.2482,   0.0993,  -1.7275],\n",
      "        [ -0.3005,  -0.2794,  -1.8973,  -0.6177]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11712\\1390480828.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11712\\1500872668.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mnext_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mnext_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mtarget_q1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq1_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mtarget_q2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq2_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mtarget_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_q1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_q2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_log_std\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rmarr\\Documents\\python-envs\\3.7.0\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11712\\1403778508.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        \n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "        \n",
    "env = GridEnv()\n",
    "agent = SACAgent(state_dim=2, action_dim=4)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.add((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if len(replay_buffer.buffer) > 1000:\n",
    "            agent.train(replay_buffer)\n",
    "    \n",
    "    print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.7.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
