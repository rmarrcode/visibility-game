                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓
 Version information:
  ml-agents: 0.27.0,
  ml-agents-envs: 0.28.0,
  Communicator API: 1.5.0,
  PyTorch: 1.7.1+cu110
[INFO] Hyperparameters for behavior name AgentA:
	trainer_type:	ppo
	hyperparameters:	
	  batch_size:	50
	  buffer_size:	500
	  learning_rate:	0.0003
	  beta:	0.005
	  epsilon:	0.2
	  lambd:	0.95
	  num_epoch:	3
	  learning_rate_schedule:	linear
	network_settings:	
	  normalize:	False
	  hidden_units:	128
	  num_layers:	2
	  vis_encode_type:	simple
	  memory:	None
	  goal_conditioning_type:	hyper
	reward_signals:	
	  extrinsic:	
	    gamma:	0.99
	    strength:	100.0
	    network_settings:	
	      normalize:	False
	      hidden_units:	128
	      num_layers:	2
	      vis_encode_type:	simple
	      memory:	None
	      goal_conditioning_type:	hyper
	init_path:	None
	keep_checkpoints:	5
	checkpoint_interval:	500000
	max_steps:	500000
	time_horizon:	64
	summary_freq:	5000
	threaded:	False
	self_play:	None
	behavioral_cloning:	None
[INFO] AgentA. Step: 5000. Time Elapsed: 35.902 s. Mean Reward: 0.429. Std of Reward: 0.495. Training.
[INFO] AgentA. Step: 10000. Time Elapsed: 57.203 s. Mean Reward: 0.786. Std of Reward: 0.410. Training.
[INFO] AgentA. Step: 15000. Time Elapsed: 79.119 s. Mean Reward: 0.667. Std of Reward: 0.471. Training.
[INFO] AgentA. Step: 20000. Time Elapsed: 101.895 s. Mean Reward: 0.778. Std of Reward: 0.416. Training.
[INFO] AgentA. Step: 25000. Time Elapsed: 182.842 s. Mean Reward: 0.800. Std of Reward: 0.400. Training.
[INFO] AgentA. Step: 30000. Time Elapsed: 204.033 s. Mean Reward: 1.000. Std of Reward: 0.000. Training.
[INFO] AgentA. Step: 35000. Time Elapsed: 224.867 s. Mean Reward: 0.957. Std of Reward: 0.202. Training.
[INFO] AgentA. Step: 40000. Time Elapsed: 247.706 s. Mean Reward: 1.000. Std of Reward: 0.000. Training.
[INFO] AgentA. Step: 45000. Time Elapsed: 269.355 s. Mean Reward: 1.000. Std of Reward: 0.000. Training.
[INFO] AgentA. Step: 50000. Time Elapsed: 290.209 s. Mean Reward: 1.000. Std of Reward: 0.000. Training.
[INFO] AgentA. Step: 55000. Time Elapsed: 311.553 s. Mean Reward: 1.000. Std of Reward: 0.000. Training.
[INFO] AgentA. Step: 60000. Time Elapsed: 332.494 s. Mean Reward: 1.000. Std of Reward: 0.000. Training.
[INFO] AgentA. Step: 65000. Time Elapsed: 353.674 s. Mean Reward: 1.000. Std of Reward: 0.000. Training.
[INFO] AgentA. Step: 70000. Time Elapsed: 374.892 s. Mean Reward: 1.000. Std of Reward: 0.000. Training.
[INFO] AgentA. Step: 75000. Time Elapsed: 396.327 s. Mean Reward: 1.000. Std of Reward: 0.000. Training.
[INFO] AgentA. Step: 80000. Time Elapsed: 417.423 s. Mean Reward: 1.000. Std of Reward: 0.000. Training.
[INFO] AgentA. Step: 85000. Time Elapsed: 438.479 s. Mean Reward: 1.000. Std of Reward: 0.000. Training.
[INFO] Learning was interrupted. Please wait while the graph is generated.
[INFO] Exported results\ppo\AgentA\AgentA-85813.onnx
[INFO] Copied results\ppo\AgentA\AgentA-85813.onnx to results\ppo\AgentA.onnx.